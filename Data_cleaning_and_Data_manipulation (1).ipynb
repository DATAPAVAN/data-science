{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Handling missing values in a Dataset:\n",
        "\n",
        "Missing data is defined as the values or data that is not stored (or not present) for some variable/s in the given dataset. Below is a sample of the missing data from the Titanic dataset. You can see the columns ‘Age’ and ‘Cabin’ have some missing values.\n",
        "\n",
        "In Pandas, usually, missing values are represented by NaN. It stands for Not a Number.\n",
        "\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "If you are aiming for a job as a data scientist, you must know how to handle the problem of missing values, which is quite common in many real-life datasets. Incomplete data can bias the results of the machine learning models and/or reduce the accuracy of the model. This article describes missing data, how it is represented, and the different reasons data values get missed. Along with the different categories of missing data, it also details out different ways of handling missing values with dataset examples.\n",
        "\n",
        "**Learning Objectives:**\n",
        "\n",
        "In this tutorial, we will learn about missing values and the benefits of missing data analysis in data science.\n",
        "You will learn about the different types of missing data and how to handle them correctly.\n",
        "You will also learn about the most widely used imputation methods to handle incomplete data.\n",
        "\n",
        "**Table of Contents:**\n",
        "\n",
        "What Is a Missing Value?\n",
        "\n",
        "How Is a Missing Value Represented in a Dataset?\n",
        "\n",
        "Why Is Data Missing From the Dataset?\n",
        "\n",
        "Types of Missing Values?\n",
        "\n",
        "Why Do We Need to Care About Handling Missing Data?\n",
        "\n",
        "How to Impute Missing Values for Categorical Features?\n",
        "\n",
        "How to Impute Missing Values Using Sci-kit Learn Library?\n",
        "\n",
        "How to Use “Missingness” as a Feature?\n",
        "\n",
        "\n",
        "\n",
        "**Why Is Data Missing From the Dataset?**\n",
        "\n",
        "There can be multiple reasons why certain values are missing from the data. Reasons for the missing of data from the dataset affect the approach of handling missing data. So it’s necessary to understand why the data could be missing.\n",
        "\n",
        "**Some of the reasons are listed below:**\n",
        "\n",
        "Past data might get corrupted due to improper maintenance.\n",
        "Observations are not recorded for certain fields due to some reasons. There might be a failure in recording the values due to human error.\n",
        "The user has not provided the values intentionally\n",
        "Item nonresponse: This means the participant refused to respond.\n",
        "Types of Missing Values\n",
        "Formally the missing values are categorized as follows:\n",
        "\n",
        "**Missing Completely At Random (MCAR):**\n",
        "\n",
        "In MCAR, the probability of data being missing is the same for all the observations. In this case, there is no relationship between the missing data and any other values observed or unobserved (the data which is not recorded) within the given dataset. That is, missing values are completely independent of other data. There is no pattern.\n",
        "\n",
        "In the case of MCAR data, the value could be missing due to human error, some system/equipment failure, loss of sample, or some unsatisfactory technicalities while recording the values. For Example, suppose in a library there are some overdue books. Some values of overdue books in the computer system are missing. The reason might be a human error, like the librarian forgetting to type in the values. So, the missing values of overdue books are not related to any other variable/data in the system. It should not be assumed as it’s a rare case. The advantage of such data is that the statistical analysis remains unbiased.\n",
        "\n",
        "**Missing At Random (MAR):**\n",
        "\n",
        "MAR data means that the reason for missing values can be explained by variables on which you have complete information, as there is some relationship between the missing data and other values/data. In this case, the data is not missing for all the observations. It is missing only within sub-samples of the data, and there is some pattern in the missing values.\n",
        "\n",
        "For example, if you check the survey data, you may find that all the people have answered their ‘Gender,’ but ‘Age’ values are mostly missing for people who have answered their ‘Gender’ as ‘female.’ (The reason being most of the females don’t want to reveal their age.)\n",
        "\n",
        "So, the probability of data being missing depends only on the observed value or data. In this case, the variables ‘Gender’ and ‘Age’ are related. The reason for missing values of the ‘Age’ variable can be explained by the ‘Gender’ variable, but you can not predict the missing value itself.\n",
        "\n",
        "Suppose a poll is taken for overdue books in a library. Gender and the number of overdue books are asked in the poll. Assume that most of the females answer the poll and men are less likely to answer. So why the data is missing can be explained by another factor, that is gender. In this case, the statistical analysis might result in bias. Getting an unbiased estimate of the parameters can be done only by modeling the missing data.\n",
        "\n",
        "**Missing Not At Random (MNAR):**\n",
        "\n",
        "Missing values depend on the unobserved data. If there is some structure/pattern in missing data and other observed data can not explain it, then it is considered to be Missing Not At Random (MNAR).\n",
        "\n",
        "If the missing data does not fall under the MCAR or MAR, it can be categorized as MNAR. It can happen due to the reluctance of people to provide the required information. A specific group of respondents may not answer some questions in a survey.\n",
        "\n",
        "For example, suppose the name and the number of overdue books are asked in the poll for a library. So most of the people having no overdue books are likely to answer the poll. People having more overdue books are less likely to answer the poll. So, in this case, the missing value of the number of overdue books depends on the people who have more books overdue.\n",
        "\n",
        "Another example is that people having less income may refuse to share some information in a survey or questionnaire.\n",
        "\n",
        "In the case of MNAR as well, the statistical analysis might result in bias.\n",
        "\n",
        "**Why Do We Need to Care About Handling Missing Data?**\n",
        "\n",
        "It is important to handle the missing values appropriately.\n",
        "\n",
        "Many machine learning algorithms fail if the dataset contains missing values. However, algorithms like K-nearest and Naive Bayes support data with missing values.\n",
        "You may end up building a biased machine learning model, leading to incorrect results if the missing values are not handled properly.\n",
        "Missing data can lead to a lack of precision in the statistical analysis.\n",
        "\n",
        "Checking for Missing Values in Python\n",
        "The first step in handling missing values is to carefully look at the complete data and find all the missing values. The following code shows the total number of missing values in each column. It also shows the total number of missing values in the entire data set.\n",
        "\n",
        "\n",
        "From the above output, we can see that there are 6 columns – Gender, Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term, and Credit_History having missing values.\n",
        "\n",
        "\n",
        "***Find the total number of missing values from the entire dataset :***\n",
        "\n",
        "train_df.isnull().sum().sum() \n",
        "\n",
        "\n",
        "Handling Missing Values\n",
        "Now that you have found the missing data, how do you handle the missing values?\n",
        "\n",
        "Analyze each column with missing values carefully to understand the reasons behind the missing of those values, as this information is crucial to choose the strategy for handling the missing values.\n",
        "\n",
        "There are 2 primary ways of handling missing values:\n",
        "\n",
        "1.Deleting the Missing values\n",
        "2.Imputing the Missing Values\n",
        "\n",
        "**Deleting the Missing value:**\n",
        "\n",
        "Generally, this approach is not recommended. It is one of the quick and dirty techniques one can use to deal with missing values. If the missing value is of the type Missing Not At Random (MNAR), then it should not be deleted.\n",
        "\n",
        "If the missing value is of type Missing At Random (MAR) or Missing Completely At Random (MCAR) then it can be deleted (In the analysis, all cases with available data are utilized, while missing observations are assumed to be completely random (MCAR) and addressed through pairwise deletion.)\n",
        "\n",
        "The disadvantage of this method is one might end up deleting some useful data from the dataset.\n",
        "\n",
        "**There are 2 ways one can delete the missing data values:**\n",
        "\n",
        "**Deleting the entire row (listwise deletion):**\n",
        "\n",
        "If a row has many missing values, you can drop the entire row. If every row has some (column) value missing, you might end up deleting the whole data. The code to drop the entire row is as follows:\n",
        "\n",
        "IN:\n",
        "df = train_df.dropna(axis=0)\n",
        "df.isnull().sum()\n",
        "OUT:\n",
        "Loan_ID  0\n",
        "Gender  0\n",
        "Married  0\n",
        "Dependents  0\n",
        "Education  0\n",
        "Self_Employed 0\n",
        "ApplicantIncome  0\n",
        "CoapplicantIncome  0\n",
        "LoanAmount  0\n",
        "Loan_Amount_Term  0\n",
        "Credit_History  0\n",
        "Property_Area  0\n",
        "Loan_Status  0\n",
        "dtype: int64\n",
        "\n",
        "**Deleting the entire column:**\n",
        "\n",
        "If a certain column has many missing values, then you can choose to drop the entire column. The code to drop the entire column is as follows:\n",
        "\n",
        "IN:\n",
        "df = train_df.drop(['Dependents'],axis=1)\n",
        "df.isnull().sum()\n",
        "\n",
        "OUT:\n",
        "Loan_ID  0\n",
        "Gender  13\n",
        "Married  3\n",
        "Education  0\n",
        "Self_Employed 32\n",
        "ApplicantIncome  0\n",
        "CoapplicantIncome  0\n",
        "LoanAmount  22\n",
        "Loan_Amount_Term  14\n",
        "Credit_History  50\n",
        "Property_Area  0\n",
        "Loan_Status  0\n",
        "dtype: int64\n",
        "\n",
        "**Imputing the Missing Value:**\n",
        "There are many imputation methods for replacing the missing values. You can use different python libraries such as Pandas, and Sci-kit Learn to do this. Let’s go through some of the ways of replacing the missing values.\n",
        "\n",
        "**Replacing with an arbitrary value:**\n",
        "\n",
        "If you can make an educated guess about the missing value, then you can replace it with some arbitrary value using the following code. E.g., in the following code, we are replacing the missing values of the ‘Dependents’ column with ‘0’.\n",
        "\n",
        "**Replace the missing value with '0' using 'fiilna' method :**\n",
        "\n",
        "train_df['Dependents'] = train_df['Dependents'].fillna(0)\n",
        "train_df[‘Dependents'].isnull().sum()\n",
        "\n",
        "OUT:\n",
        "0\n",
        "\n",
        "**Replacing with the mean:**\n",
        "\n",
        "This is the most common method of imputing missing values of numeric columns. If there are outliers, then the mean will not be appropriate. In such cases, outliers need to be treated first. You can use the ‘fillna’ method for imputing the columns ‘LoanAmount’ and ‘Credit_History’ with the mean of the respective column values.\n",
        "\n",
        "\n",
        "**Replace the missing values for numerical columns with mean :**\n",
        "\n",
        "train_df['LoanAmount'] = train_df['LoanAmount'].fillna(train_df['LoanAmount'].mean())\n",
        "train_df['Credit_History'] = train_df[‘Credit_History'].fillna(train_df['Credit_History'].mean())\n",
        "\n",
        "OUT:\n",
        "Loan_ID  0\n",
        "Gender  13\n",
        "Married  3\n",
        "Dependents  15\n",
        "Education  0\n",
        "Self_Employed 32\n",
        "ApplicantIncome  0\n",
        "CoapplicantIncome  0\n",
        "LoanAmount 0\n",
        "Loan_Amount_Term 0\n",
        "Credit_History 0\n",
        "Property_Area  0\n",
        "Loan_Status  0\n",
        "dtype: int64\n",
        "\n",
        "**Replacing with the mode:**\n",
        "\n",
        "Mode is the most frequently occurring value. It is used in the case of categorical features. You can use the ‘fillna’ method for imputing the categorical columns ‘Gender,’ ‘Married,’ and ‘Self_Employed.’\n",
        "\n",
        "IN:\n",
        "\n",
        "**Replace the missing values for categorical columns with mode:**\n",
        "train_df['Gender'] = train_df['Gender'].fillna(train_df['Gender'].mode()[0])\n",
        "train_df['Married'] = train_df['Married'].fillna(train_df['Married'].mode()[0])\n",
        "train_df['Self_Employed'] = train_df[‘Self_Employed'].fillna(train_df['Self_Employed'].mode()[0])\n",
        "train_df.isnull().sum()\n",
        "\n",
        "OUT:\n",
        "Loan_ID 0\n",
        "Gender  0\n",
        "Married 0\n",
        "Dependents  0\n",
        "Education 0\n",
        "Self_Employed 0\n",
        "ApplicantIncome 0\n",
        "CoapplicantIncome 0\n",
        "LoanAmount  0\n",
        "Loan_Amount_Term  0\n",
        "Credit_History  0\n",
        "Property_Area 0\n",
        "Loan_Status 0\n",
        "dtype: int64\n",
        "\n",
        "**Replacing with the median:**\n",
        "\n",
        "The median is the middlemost value. It’s better to use the median value for imputation in the case of outliers. You can use the ‘fillna’ method for imputing the column ‘Loan_Amount_Term’ with the median value.\n",
        "\n",
        "train_df['Loan_Amount_Term']= train_df['Loan_Amount_Term'].fillna(train_df['Loan_Amount_Term'].median())\n",
        "Replacing with the previous value – forward fill\n",
        "\n",
        "In some cases, imputing the values with the previous value instead of the mean, mode, or median is more appropriate. This is called forward fill. It is mostly used in time series data. You can use the ‘fillna’ function with the parameter ‘method = ffill’\n",
        "\n",
        "IN:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "test = pd.Series(range(6))\n",
        "test.loc[2:4] = np.nan\n",
        "test\n",
        "OUT:\n",
        "0 0.0\n",
        "1 1.0\n",
        "2 Nan\n",
        "3 Nan\n",
        "4 Nan\n",
        "5 5.0\n",
        "dtype: float64\n",
        "IN:\n",
        "**Forward-Fill:**\n",
        "\n",
        "test.fillna(method=‘ffill')\n",
        "OUT:\n",
        "0 0.0\n",
        "1 1.0\n",
        "2 1.0\n",
        "3 1.0\n",
        "4 1.0\n",
        "5 5.0\n",
        "dtype: float64\n",
        "Replacing with the next value – backward fill\n",
        "\n",
        "In backward fill, the missing value is imputed using the next value.\n",
        "\n",
        "\n",
        "**Backward-Fill :**\n",
        "\n",
        "test.fillna(method=‘bfill')\n",
        "OUT:\n",
        "0 0.0\n",
        "1 1.0\n",
        "2 5.0\n",
        "3 5.0\n",
        "4 5.0\n",
        "5 5.0\n",
        "dtype: float64\n",
        "\n",
        "**Interpolation:**\n",
        "\n",
        "Missing values can also be imputed using interpolation. Pandas’ interpolate method can be used to replace the missing values with different interpolation methods like ‘polynomial,’ ‘linear,’ and ‘quadratic.’ The default method is ‘linear.’\n",
        "\n",
        "IN:\n",
        "test.interpolate()\n",
        "OUT:\n",
        "0 0.0\n",
        "1 1.0\n",
        "2 2.0\n",
        "3 3.0\n",
        "4 4.0\n",
        "5 5.0\n",
        "dtype: float64\n",
        "How to Impute Missing Values for Categorical Features?\n",
        "There are two ways to impute missing values for categorical features as follows:\n",
        "\n",
        "**Impute the Most Frequent Value:**\n",
        "\n",
        "We will use ‘SimpleImputer’ in this case, and as this is a non-numeric column, we can’t use mean or median, but we can use the most frequent value and constant.\n",
        "\n",
        "IN:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "X = pd.DataFrame({'Shape':['square', 'square', 'oval', 'circle', np.nan]})\n",
        "X\n",
        "Shape\n",
        "OUT:\n",
        "0 square\n",
        "1 square\n",
        "2 oval\n",
        "3 circle\n",
        "4 NaN\n",
        "IN:\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "imputer.fit_transform(X)\n",
        "OUT:\n",
        "array([['square'],\n",
        "       ['square'],\n",
        "       ['oval'],\n",
        "       ['circle'],\n",
        "       ['square']], dtype=object)\n",
        "As you can see, the missing value is imputed with the most frequent value, ’square.’\n",
        "\n",
        "**Impute the Value “Missing”:**\n",
        "\n",
        "We can impute the value “missing,” which treats it as a separate category.\n",
        "\n",
        "IN:\n",
        "imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
        "imputer.fit_transform(X)\n",
        "OUT:\n",
        "array([['square'],\n",
        "       ['square'],\n",
        "       ['oval'],\n",
        "       ['circle'],\n",
        "       ['missing']], dtype=object)\n",
        "In any of the above approaches, you will still need to OneHotEncode the data (or you can also use another encoder of your choice). After One Hot Encoding, in case 1, instead of the values ‘square,’ ‘oval,’ and’ circle,’ you will get three feature columns. And in case 2, you will get four feature columns (4th one for the ‘missing’ category). So it’s like adding the missing indicator column in the data. There is another way to add a missing indicator column, which we will discuss further.\n",
        "\n",
        "**How to Impute Missing Values Using Sci-kit Learn Library?**\n",
        "\n",
        "We can impute missing values using the sci-kit library by creating a model to predict the observed value of a variable based on another variable which is known as regression imputation.\n",
        "\n",
        "**Univariate Approach:**\n",
        "\n",
        "In a Univariate approach, only a single feature is taken into consideration. You can use the class SimpleImputer and replace the missing values with mean, mode, median, or some constant value.\n",
        "\n",
        "Let’s see an example:\n",
        "\n",
        "IN:\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
        "OUT: SimpleImputer()\n",
        "IN:\n",
        "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
        "print(imp.transform(X))\n",
        "OUT:\n",
        "[[4.          2.        ]\n",
        " [6.          3.666...]\n",
        " [7.          6.        ]]\n",
        "**Multivariate Approach:**\n",
        "\n",
        "In a multivariate approach, more than one feature is taken into consideration. There are two ways to impute missing values considering the multivariate approach. Using KNNImputer or IterativeImputer classes.\n",
        "\n",
        "Let’s take an example of a titanic dataset.\n",
        "\n",
        "Suppose the feature ‘age’ is well correlated with the feature ‘Fare’ such that people with lower fares are also younger and people with higher fares are also older. In that case, it would make sense to impute low age for low fare values and high age for high fare values. So here, we are taking multiple features into account by following a multivariate approach.\n",
        "\n",
        "IN:\n",
        "import pandas as pd\n",
        "df = pd.read_csv('http://bit.ly/kaggletrain', nrows=6)\n",
        "cols = ['SibSp', 'Fare', 'Age']\n",
        "X = df[cols]\n",
        "X\n",
        "SibSp\tFare\tAge\n",
        "0\t1\t7.2500\t22.0\n",
        "1\t1\t71.2833\t38.0\n",
        "2\t0\t7.9250\t26.0\n",
        "3\t1\t53.1000\t35.0\n",
        "4\t0\t8.0500\t35.0\n",
        "5\t0\t8.4583\tNaN\n",
        "IN:\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "impute_it = IterativeImputer()\n",
        "impute_it.fit_transform(X)\n",
        "OUT:\n",
        "array([[ 1.        ,  7.25      , 22.        ],\n",
        "       [ 1.        , 71.2833    , 38.        ],\n",
        "       [ 0.        ,  7.925     , 26.        ],\n",
        "       [ 1.        , 53.1       , 35.        ],\n",
        "       [ 0.        ,  8.05      , 35.        ],\n",
        "       [ 0.        ,  8.4583    , 28.50639495]])\n",
        "Let’s see how IterativeImputer works. For all rows in which ‘Age’ is not missing, sci-kit learn runs a regression model. It uses ‘Sib sp’ and ‘Fare’ as the features and ‘Age’ as the target. And then, for all rows for which ‘Age’ is missing, it makes predictions for ‘Age’ by passing ‘Sib sp’ and ‘Fare’ to the training model. So it actually builds a regression model with two features and one target and then makes predictions on any places where there are missing values. And those predictions are the imputed values.\n",
        "\n",
        "**Nearest Neighbors Imputations (KNNImputer):**\n",
        "\n",
        "Missing values are imputed using the k-Nearest Neighbors approach, where a Euclidean distance is used to find the nearest neighbors. Let’s take the above example of the titanic dataset to see how it works.\n",
        "\n",
        "IN:\n",
        "from sklearn.impute import KNNImputer\n",
        "impute_knn = KNNImputer(n_neighbors=2)\n",
        "impute_knn.fit_transform(X)\n",
        "OUT:\n",
        "array([[ 1.    ,  7.25  , 22.    ],\n",
        "       [ 1.    , 71.2833, 38.    ],\n",
        "       [ 0.    ,  7.925 , 26.    ],\n",
        "       [ 1.    , 53.1   , 35.    ],\n",
        "       [ 0.    ,  8.05  , 35.    ],\n",
        "       [ 0.    ,  8.4583, 30.5   ]])\n",
        "In the above example, the n_neighbors=2. So sci-kit learn finds the two most similar rows measured by how close the ‘Sib sp’ and ‘Fare’ values are to the row which has missing values. In this case, the last row has a missing value. And the third row and the fifth row have the closest values for the other two features. So the average of the ‘Age’ feature from these two rows is taken as the imputed value.\n",
        "\n",
        "How to Use “Missingness” as a Feature?\n",
        "In some cases, while imputing missing values, you can preserve information about which values were missing and use that as a feature. This is because sometimes, there may be a relationship between the reason for missing values (also called the “missingness”) and the target variable you are trying to predict. In such cases, you can add a missing indicator to encode the “missingness” as a feature in the imputed data set.\n",
        "\n",
        "Where can we use this?\n",
        "\n",
        "Suppose you are predicting the presence of a disease. Now, imagine a scenario where a missing age is a good predictor of the disease because we don’t have records for people in poverty. The age values are not missing at random. They are missing for people in poverty, and poverty is a good predictor of disease. Thus, missing age or “missingness” is a good predictor of disease.\n",
        "\n",
        "IN:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "X = pd.DataFrame({'Age':[20, 30, 10, np.nan, 10]})\n",
        "X\n",
        "Age\n",
        "0\t20.0\n",
        "1\t30.0\n",
        "2\t10.0\n",
        "3\tNaN\n",
        "4\t10.0\n",
        "IN:\n",
        "from sklearn.impute\n",
        "import SimpleImputer\n",
        "\n",
        "**impute the mean**\n",
        "\n",
        "imputer = SimpleImputer()\n",
        "imputer.fit_transform(X)\n",
        "OUT:\n",
        "array([[20. ],\n",
        "       [30. ],\n",
        "       [10. ],\n",
        "       [17.5],\n",
        "       [10. ]])\n",
        "\n",
        "IN:\n",
        "imputer = SimpleImputer(add_indicator=True)\n",
        "imputer.fit_transform(X)\n",
        "OUT:\n",
        "array([[20. ,  0. ],\n",
        "       [30. ,  0. ],\n",
        "       [10. ,  0. ],\n",
        "       [17.5,  1. ],\n",
        "       [10. ,  0. ]])\n",
        "In the above example, the second column indicates whether the corresponding value in the first column was missing or not. ‘1’ indicates that the corresponding value was missing, and ‘0’ indicates that the corresponding value was not missing.\n",
        "\n",
        "If you don’t want to impute missing values but only want to have the indicator matrix, then you can use the ‘MissingIndicator’ class from scikit learn.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Missing data is a problem everyone faces while dealing with real-life data. It can impact the quality and accuracy of our results. Understanding the different types of missing data values and their potential impact on the analysis is crucial for researchers to select an appropriate method for handling the missing data. Each method has its advantages and disadvantages and is appropriate for different types of missing data values.\n"
      ],
      "metadata": {
        "id": "GgGliinQ2Ku-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Handling Duplicates in a data set:\n",
        "                                            \n",
        "\n",
        "\n",
        "**Duplicates in a structured dataset:**\n",
        "\n",
        "Duplicates in a structured data set can cause issues with data quality, model performance, and interpretation of results. Here are some common methods to deal with duplicates:\n",
        "\n",
        "**Drop Duplicates:**\n",
        "\n",
        " One straightforward approach is to drop all duplicate rows in the data set. This can be done using the \"drop_duplicates\" function in Python's pandas library or the \"REMOVE DUPLICATES\" command in SQL. However, this approach can lead to loss of valuable data if the duplicates contain unique information.\n",
        "\n",
        "**Aggregate Duplicates:**\n",
        "\n",
        " Another approach is to aggregate the data by combining the duplicate rows into a single row. This can be done using functions such as \"groupby\" and \"agg\" in pandas or using SQL commands such as \"GROUP BY\" and \"SELECT ... COUNT()\". Aggregation can be performed by computing the mean, median, mode, or sum of the values in each column, depending on the data type and the research question.\n",
        "\n",
        "**Identify and Resolve Duplicates:**\n",
        "\n",
        " Sometimes, duplicates may be the result of errors or inconsistencies in the data collection process. In this case, it may be necessary to identify the root cause of the duplicates and resolve them manually. This can involve data cleaning, data validation, or merging data from different sources.\n",
        "\n",
        "**Record Linkage:**\n",
        "\n",
        " Record linkage is a process that involves identifying and linking records that refer to the same entity across different data sources. Record linkage techniques can be used to identify and remove duplicates in a structured data set by matching records based on common attributes such as name, address, or phone number.\n",
        "\n",
        "**Data Fusion:**\n",
        "\n",
        " Data fusion is a process that involves combining data from multiple sources to create a more complete and accurate representation of the underlying phenomenon. Data fusion techniques can be used to merge duplicate records from different data sets, while also resolving any inconsistencies or errors in the data.\n",
        "\n",
        "\n",
        "**Handling Duplicates In An Unstructured Dataset:**\n",
        "\n",
        "Dealing with duplicates in an unstructured data set can be more challenging than in a structured data set because unstructured data typically lacks a consistent structure or format. Here are some common methods to deal with duplicates in an unstructured data set:\n",
        "\n",
        "**Text Similarity:**\n",
        "\n",
        " One approach to identify duplicates in an unstructured data set is to compute the similarity between texts using natural language processing (NLP) techniques. Similarity measures such as cosine similarity, Jaccard similarity, or Levenshtein distance can be used to identify texts that have a high degree of similarity. Once the duplicates are identified, they can be removed or merged.\n",
        "\n",
        "**Hashing:**\n",
        "\n",
        " Hashing is a technique that involves generating a unique digital fingerprint or hash for each text. Hashing can be used to identify texts that have the same hash value, indicating that they are duplicates. Hashing can be performed using algorithms such as MD5, SHA1, or SHA256.\n",
        "\n",
        "**Clustering:**\n",
        "\n",
        " Clustering is a machine learning technique that involves grouping similar texts into clusters. Clustering algorithms such as k-means, hierarchical clustering, or density-based clustering can be used to group texts that are similar based on their content, structure, or metadata. Once the clusters are formed, the duplicates can be identified and removed or merged.\n",
        "\n",
        "**Record Linkage:**\n",
        "\n",
        " Record linkage techniques can also be applied to unstructured data sets to identify and link records that refer to the same entity. Record linkage can be performed using probabilistic matching algorithms that take into account the similarity between texts, as well as other metadata such as date, time, or location.\n",
        "\n",
        "Manual Review: In some cases, it may be necessary to manually review the unstructured data to identify and resolve duplicates. This can involve reading through the texts and identifying duplicates based on their content, structure, or metadata.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hvuihA-FHyfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here’s All you Need to Know About Encoding Categorical Data :\n",
        "\n",
        "\n",
        "**Overview:**\n",
        "\n",
        "Understand what is Categorical Data Encoding\n",
        "Learn different encoding techniques and when to use them\n",
        " \n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "The performance of a machine learning model not only depends on the model and the hyperparameters but also on how we process and feed different types of variables to the model. Since most machine learning models only accept numerical variables, preprocessing the categorical variables becomes a necessary step. We need to convert these categorical variables to numbers such that the model is able to understand and extract valuable information.\n",
        "\n",
        "**Categorical Data Encoding:**\n",
        "\n",
        "A typical data scientist spends 70 – 80% of his time cleaning and preparing the data. And converting categorical data is an unavoidable activity. It not only elevates the model quality but also helps in better feature engineering. Now the question is, how do we proceed? \n",
        "\n",
        "**Which Categorical data encoding method should we use?**\n",
        "\n",
        "In this article, I will be explaining various types of categorical data encoding methods with implementation in Python.\n",
        "\n",
        "In case you want to learn concepts of data science in video format, check out our course- Introduction to Data Science\n",
        "\n",
        "\n",
        "**Table of content:**\n",
        "\n",
        "What is Categorical Data?\n",
        "\n",
        "1.Label Encoding or Ordinal Encoding\n",
        "\n",
        "2.One hot Encoding\n",
        "\n",
        "3.Dummy Encoding\n",
        "\n",
        "4.Effect Encoding\n",
        "\n",
        "5.Binary Encoding\n",
        "\n",
        "6.BaseN Encoding\n",
        "\n",
        "7.Hash Encoding\n",
        "\n",
        "8.Target Encoding\n",
        " \n",
        "\n",
        "\n",
        "**What is categorical data?**\n",
        "\n",
        "Since we are going to be working on categorical variables in this article, here is a quick refresher on the same with a couple of examples. Categorical variables are usually represented as ‘strings’ or ‘categories’ and are finite in number. \n",
        "\n",
        "**Here are a few examples:**\n",
        "\n",
        "The city where a person lives: Delhi, Mumbai, Ahmedabad, Bangalore, etc.\n",
        "The department a person works in: Finance, Human resources, IT, Production.\n",
        "The highest degree a person has: High school, Diploma, Bachelors, Masters, PhD.\n",
        "The grades of a student:  A+, A, B+, B, B- etc.\n",
        "In the above examples, the variables only have definite possible values.\n",
        "\n",
        " Further, we can see there are two kinds of categorical data-\n",
        "\n",
        "**Ordinal Data**: The categories have an inherent order.\n",
        "**Nominal Data**: The categories do not have an inherent order.\n",
        "\n",
        "In Ordinal data, while encoding, one should retain the information regarding the order in which the category is provided. Like in the above example the highest degree a person possesses, gives vital information about his qualification. The degree is an important feature to decide whether a person is suitable for a post or not.\n",
        "\n",
        "While encoding Nominal data, we have to consider the presence or absence of a feature. In such a case, no notion of order is present. For example, the city a person lives in. For the data, it is important to retain where a person lives. Here, We do not have any order or sequence. It is equal if a person lives in Delhi or Bangalore.\n",
        "\n",
        "For encoding categorical data, we have a python package category_encoders. The following code helps you install easily.\n",
        "\n",
        "pip install category_encoders\n",
        " \n",
        "\n",
        "\n",
        "**Label Encoding or Ordinal Encoding:**\n",
        "\n",
        "We use this categorical data encoding technique when the categorical feature is ordinal. In this case, retaining the order is important. Hence encoding should reflect the sequence.\n",
        "\n",
        "In Label encoding, each label is converted into an integer value. We will create a variable that contains the categories representing the education qualification of a person.\n",
        "\n",
        "Python Code:\n",
        "\n",
        "\n",
        "Fit and transform train data\n",
        "\n",
        "df_train_transformed = encoder.fit_transform(train_df)\n",
        "categorical data encoding: Ordinal encoding\n",
        "\n",
        " \n",
        "\n",
        "**One Hot Encoding:**\n",
        "\n",
        "We use this categorical data encoding technique when the features are nominal(do not have any order). In one hot encoding, for each level of a categorical feature, we create a new variable. Each category is mapped with a binary variable containing either 0 or 1. Here, 0 represents the absence, and 1 represents the presence of that category.\n",
        "\n",
        "These newly created binary features are known as Dummy variables. The number of dummy variables depends on the levels present in the categorical variable. This might sound complicated. Let us take an example to understand this better. Suppose we have a dataset with a category animal, having different animals like Dog, Cat, Sheep, Cow, Lion. Now we have to one-hot encode this data.\n",
        "\n",
        "categorical data encoding - One hot encoding\n",
        "\n",
        "After encoding, in the second table, we have dummy variables each representing a category in the feature Animal. Now for each category that is present, we have 1 in the column of that category and 0 for the others. Let’s see how to implement a one-hot encoding in python.\n",
        "\n",
        " \n",
        "\n",
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "data=pd.DataFrame({'City':[\n",
        "'Delhi','Mumbai','Hydrabad','Chennai','Bangalore','Delhi','Hydrabad','Bangalore','Delhi'\n",
        "]})\n",
        "\n",
        "**Create object for one-hot encoding**\n",
        "\n",
        "encoder=ce.OneHotEncoder(cols='City',handle_unknown='return_nan',return_df=True,use_cat_names=True)\n",
        "\n",
        "**Original Data**\n",
        "\n",
        "data\n",
        " \n",
        "\n",
        "Categorical Data Encoding : Data\n",
        "\n",
        "**Fit and transform Data**\n",
        "\n",
        "data_encoded = encoder.fit_transform(data)\n",
        "data_encoded\n",
        "Categorical Data Encoding : One-Hot Encoding\n",
        "\n",
        "Now let’s move to another very interesting and widely used encoding technique i.e Dummy encoding.\n",
        "\n",
        " \n",
        "\n",
        "**Dummy Encoding:**\n",
        "\n",
        "Dummy coding scheme is similar to one-hot encoding. This categorical data encoding method transforms the categorical variable into a set of binary variables (also known as dummy variables). In the case of one-hot encoding, for N categories in a variable, it uses N binary variables. The dummy encoding is a small improvement over one-hot-encoding. Dummy encoding uses N-1 features to represent N labels/categories.\n",
        "\n",
        "To understand this better let’s see the image below. Here we are coding the same data using both one-hot encoding and dummy encoding techniques. While one-hot uses 3 variables to represent the data whereas dummy encoding uses 2 variables to code 3 categories.\n",
        "\n",
        "Categorical data encoding - Dummy Code\n",
        "\n",
        " \n",
        "\n",
        "Let us implement it in python.\n",
        "\n",
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "data=pd.DataFrame({'City':['Delhi','Mumbai','Hyderabad','Chennai','Bangalore','Delhi,'Hyderabad']})\n",
        "\n",
        "**Original Data:**\n",
        "\n",
        "data\n",
        "\n",
        "**Encode the data:**\n",
        "data_encoded=pd.get_dummies(data=data,drop_first=True)\n",
        "data_encoded\n",
        "\n",
        "\n",
        "Here using drop_first  argument, we are representing the first label Bangalore using 0.\n",
        "\n",
        "**Drawbacks of  One-Hot and Dummy Encoding:**\n",
        "\n",
        "One hot encoder and dummy encoder are two powerful and effective encoding schemes. They are also very popular among the data scientists, But may not be as effective when-\n",
        "\n",
        "A large number of levels are present in data. If there are multiple categories in a feature variable in such a case we need a similar number of dummy variables to encode the data. For example, a column with 30 different values will require 30 new variables for coding.\n",
        "If we have multiple categorical features in the dataset similar situation will occur and again we will end to have several binary features each representing the categorical feature and their multiple categories e.g a dataset having 10 or more categorical columns.\n",
        "In both the above cases, these two encoding schemes introduce sparsity in the dataset i.e several columns having 0s and a few of them having 1s. In other words, it creates multiple dummy features in the dataset without adding much information.\n",
        "\n",
        "Also, they might lead to a Dummy variable trap. It is a phenomenon where features are highly correlated. That means using the other variables, we can easily predict the value of a variable.\n",
        "\n",
        "Due to the massive increase in the dataset, coding slows down the learning of the model along with deteriorating the overall performance that ultimately makes the model computationally expensive. Further, while using tree-based models these encodings are not an optimum choice.\n",
        "\n",
        " \n",
        "\n",
        "**Effect Encoding:**\n",
        "\n",
        "This encoding technique is also known as Deviation Encoding or Sum Encoding. Effect encoding is almost similar to dummy encoding, with a little difference. In dummy coding, we use 0 and 1 to represent the data but in effect encoding, we use three values i.e. 1,0, and -1.\n",
        "\n",
        "The row containing only 0s in dummy encoding is encoded as -1 in effect encoding.  In the dummy encoding example, the city Bangalore at index 4  was encoded as 0000. Whereas in effect encoding it is represented by -1-1-1-1.\n",
        "\n",
        "Let us see how we implement it in python-\n",
        "\n",
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "data=pd.DataFrame({'City':['Delhi','Mumbai','Hyderabad','Chennai','Bangalore','Delhi,'Hyderabad']}) encoder=ce.sum_coding.SumEncoder(cols='City',verbose=False,)\n",
        "\n",
        "**Original Data**\n",
        "\n",
        "data\n",
        "Categorical Data Encoding: Effect Encoding\n",
        "\n",
        " \n",
        "\n",
        "encoder.fit_transform(data)\n",
        "\n",
        "\n",
        "Effect encoding is an advanced technique. In case you are interested to know more about effect encoding, refer to this interesting paper.\n",
        "\n",
        " \n",
        "\n",
        "**Hash Encoder:**\n",
        "\n",
        "To understand Hash encoding it is necessary to know about hashing. Hashing is the transformation of arbitrary size input in the form of a fixed-size value. We use hashing algorithms to perform hashing operations i.e to generate the hash value of an input. Further, hashing is a one-way process, in other words, one can not generate original input from the hash representation.\n",
        "\n",
        "Hashing has several applications like data retrieval, checking data corruption, and in data encryption also. We have multiple hash functions available for example Message Digest (MD, MD2, MD5), Secure Hash Function (SHA0, SHA1, SHA2), and many more.\n",
        "\n",
        "Just like one-hot encoding, the Hash encoder represents categorical features using the new dimensions. Here, the user can fix the number of dimensions after transformation using n_component argument. Here is what I mean – A feature with 5 categories can be represented using N new features similarly, a feature with 100 categories can also be transformed using N new features. Doesn’t this sound amazing?\n",
        "\n",
        "By default, the Hashing encoder uses the md5 hashing algorithm but a user can pass any algorithm of his choice. If you want to explore the md5 algorithm, I suggest this paper.\n",
        "\n",
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "\n",
        "**Create the dataframe**\n",
        "\n",
        "data=pd.DataFrame({'Month':['January','April','March','April','Februay','June','July','June','September']})\n",
        "\n",
        "**Create object for hash encoder**\n",
        "\n",
        "encoder=ce.HashingEncoder(cols='Month',n_components=6)\n",
        "Cateforical Data Encoding : Hash Encoder Data\n",
        "\n",
        "**Fit and Transform Data**\n",
        "\n",
        "encoder.fit_transform(data)\n",
        "Cateforical Data Encoding : Hash Encoder\n",
        "\n",
        " \n",
        "\n",
        "Since Hashing transforms the data in lesser dimensions, it may lead to loss of information. Another issue faced by hashing encoder is the collision. Since here, a large number of features are depicted into lesser dimensions, hence multiple values can be represented by the same hash value, this is known as a collision.\n",
        "\n",
        "Moreover, hashing encoders have been very successful in some Kaggle competitions. It is great to try if the dataset has high cardinality features.\n",
        "\n",
        " \n",
        "\n",
        "**Binary Encoding:**\n",
        "\n",
        "Binary encoding is a combination of Hash encoding and one-hot encoding. In this encoding scheme, the categorical feature is first converted into numerical using an ordinal encoder. Then the numbers are transformed in the binary number. After that binary value is split into different columns.\n",
        "\n",
        "Binary encoding works really well when there are a high number of categories. For example the cities in a country where a company supplies its products.\n",
        "\n",
        "**Import the libraries**\n",
        "\n",
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "\n",
        "**Create the Dataframe**\n",
        "\n",
        "data=pd.DataFrame({'City':['Delhi','Mumbai','Hyderabad','Chennai','Bangalore','Delhi','Hyderabad','Mumbai','Agra']})\n",
        "\n",
        "**Create object for binary encoding**\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['city'],return_df=True)\n",
        "\n",
        "**Original Data**\n",
        "\n",
        "data\n",
        "\n",
        "**Fit and Transform Data**\n",
        "\n",
        "data_encoded=encoder.fit_transform(data) \n",
        "data_encoded\n",
        "\n",
        "\n",
        "Binary encoding is a memory-efficient encoding scheme as it uses fewer features than one-hot encoding. Further, It reduces the curse of dimensionality for data with high cardinality.\n",
        "\n",
        " \n",
        "\n",
        "**Base N Encoding:**\n",
        "\n",
        "Before diving into BaseN encoding let’s first try to understand what is Base here?\n",
        "\n",
        "In the numeral system, the Base or the radix is the number of digits or a combination of digits and letters used to represent the numbers. The most common base we use in our life is 10  or decimal system as here we use 10 unique digits i.e 0 to 9 to represent all the numbers. Another widely used system is binary i.e. the base is 2. It uses 0 and 1 i.e 2 digits to express all the numbers.\n",
        "\n",
        "For Binary encoding, the Base is 2 which means it converts the numerical values of a category into its respective Binary form. If you want to change the Base of encoding scheme you may use Base N encoder. In the case when categories are more and binary encoding is not able to handle the dimensionality then we can use a larger base such as 4 or 8.\n",
        "\n",
        "**Import the libraries**\n",
        "\n",
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "\n",
        "**Create the dataframe**\n",
        "\n",
        "data=pd.DataFrame({'City':['Delhi','Mumbai','Hyderabad','Chennai','Bangalore','Delhi','Hyderabad','Mumbai','Agra']})\n",
        "\n",
        "**Create an object for Base N Encoding\n",
        "encoder= ce.BaseNEncoder(cols=['city'],return_df=True,base=5)\n",
        "\n",
        "**Original Data**\n",
        "\n",
        "data\n",
        "\n",
        "**Fit and Transform Data**\n",
        "\n",
        "data_encoded=encoder.fit_transform(data)\n",
        "data_encoded\n",
        "\n",
        "\n",
        "In the above example, I have used base 5 also known as the Quinary system. It is similar to the example of Binary encoding. While Binary encoding represents the same data by 4 new features the BaseN encoding uses only 3 new variables.\n",
        "\n",
        "Hence BaseN encoding technique further reduces the number of features required to efficiently represent the data and improving memory usage. The default Base for Base N is 2 which is equivalent to Binary Encoding.\n",
        "\n",
        " \n",
        "\n",
        "**Target Encoding:**\n",
        "\n",
        "Target encoding is a Baysian encoding technique.\n",
        "\n",
        "Bayesian encoders use information from dependent/target variables to encode the categorical data.\n",
        "\n",
        "In target encoding, we calculate the mean of the target variable for each category and replace the category variable with the mean value. In the case of the categorical target variables, the posterior probability of the target replaces each category..\n",
        "\n",
        "**import the libraries**\n",
        "\n",
        "import pandas as pd\n",
        "import category_encoders as ce\n",
        "\n",
        "**Create the Datafram**\n",
        "\n",
        "data=pd.DataFrame({'class':['A,','B','C','B','C','A','A','A'],'Marks':[50,30,70,80,45,97,80,68]})\n",
        "\n",
        "**Create target encoding object**\n",
        "\n",
        "encoder=ce.TargetEncoder(cols='class') \n",
        "\n",
        "**Original Data**\n",
        "\n",
        "Data\n",
        "categorical_data_encoding: data\n",
        "\n",
        "**Fit and Transform Train Data**\n",
        "\n",
        "encoder.fit_transform(data['class'],data['Marks'])\n",
        "categorical_data_encoding: Target encoding\n",
        "\n",
        "We perform Target encoding for train data only and code the test data using results obtained from the training dataset. Although, a very efficient coding system, it has the following issues responsible for deteriorating the model performance-\n",
        "\n",
        "It can lead to target leakage or overfitting. To address overfitting we can use different techniques.\n",
        "In the leave one out encoding, the current target value is reduced from the overall mean of the target to avoid leakage.\n",
        "In another method, we may introduce some Gaussian noise in the target statistics. The value of this noise is hyperparameter to the model.\n",
        "The second issue, we may face is the improper distribution of categories in train and test data. In such a case, the categories may assume extreme values. Therefore the target means for the category are mixed with the marginal mean of the target"
      ],
      "metadata": {
        "id": "NxKF9G1YJFd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Handling Outliers in a Dataset:\n",
        "\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "One of the most important steps as part of data preprocessing is detecting and treating the outliers as they can negatively affect the statistical analysis and the training process of a machine learning algorithm resulting in lower accuracy.\n",
        "\n",
        "**1. What are Outliers?**\n",
        "\n",
        "We all have heard of the idiom ‘odd one out which means something unusual in comparison to the others in a group.\n",
        "\n",
        "Similarly, an Outlier is an observation in a given dataset that lies far from the rest of the observations. That means an outlier is vastly larger or smaller than the remaining values in the set.\n",
        "\n",
        "**2. Why do they occur?**\n",
        "\n",
        "An outlier may occur due to the variability in the data, or due to experimental error/human error.\n",
        "\n",
        "They may indicate an experimental error or heavy skewness in the data(heavy-tailed distribution).\n",
        "\n",
        "**3. What do they affect?**\n",
        "\n",
        "In statistics, we have three measures of central tendency namely Mean, Median, and Mode. They help us describe the data.\n",
        "\n",
        "Mean is the accurate measure to describe the data when we do not have any outliers present.\n",
        "\n",
        "Median is used if there is an outlier in the dataset.\n",
        "\n",
        "Mode is used if there is an outlier AND about ½ or more of the data is the same.\n",
        "\n",
        "‘Mean’ is the only measure of central tendency that is affected by the outliers which in turn impacts Standard deviation.\n",
        "\n",
        "Example:\n",
        "Consider a small dataset, sample= [15, 101, 18, 7, 13, 16, 11, 21, 5, 15, 10, 9]. By looking at it, one can quickly say ‘101’ is an outlier that is much larger than the other values.\n",
        "\n",
        "small dataset\n",
        "computation with and without outlier (Image by author)\n",
        "From the above calculations, we can clearly say the Mean is more affected than the Median.\n",
        "\n",
        "**4. Detecting Outliers:**\n",
        "\n",
        "If our dataset is small, we can detect the outlier by just looking at the dataset. But what if we have a huge dataset, how do we identify the outliers then? We need to use visualization and mathematical techniques.\n",
        "\n",
        "Below are some of the techniques of detecting outliers\n",
        "\n",
        "1.Boxplots\n",
        "2.Z-score\n",
        "3.Inter Quantile Range(IQR)\n",
        "\n",
        "**4.1 Detecting outliers using Boxplot:**\n",
        "\n",
        "Python code for boxplot is:\n",
        "\n",
        "\n",
        "**4.2 Detecting outliers using the Z-scores\n",
        "Criteria:**\n",
        "\n",
        "any data point whose Z-score falls out of 3rd standard deviation is an outlier.\n",
        "\n",
        "Detecting Outliers with Z-scores.\n",
        "\n",
        "Steps:\n",
        "loop through all the data points and compute the Z-score using the formula (Xi-mean)/std.\n",
        "define a threshold value of 3 and mark the datapoints whose absolute value of Z-score is greater than the threshold as outliers.\n",
        "import numpy as np\n",
        "outliers = []\n",
        "def detect_outliers_zscore(data):\n",
        "    thres = 3\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    # print(mean, std)\n",
        "    for i in data:\n",
        "        z_score = (i-mean)/std\n",
        "        if (np.abs(z_score) > thres):\n",
        "            outliers.append(i)\n",
        "    return outliers# Driver code\n",
        "sample_outliers = detect_outliers_zscore(sample)\n",
        "print(\"Outliers from Z-scores method: \", sample_outliers)\n",
        "The above code outputs: Outliers from Z-scores method: [101]\n",
        "\n",
        "**4.3 Detecting outliers using the Inter Quantile Range(IQR):**\n",
        "\n",
        "steps:\n",
        "Sort the dataset in ascending order\n",
        "calculate the 1st and 3rd quartiles(Q1, Q3)\n",
        "compute IQR=Q3-Q1\n",
        "compute lower bound = (Q1–1.5*IQR), upper bound = (Q3+1.5*IQR)\n",
        "loop through the values of the dataset and check for those who fall below the lower bound and above the upper bound and mark them as outliers\n",
        "Python Code:\n",
        "\n",
        "outliers = []\n",
        "def detect_outliers_iqr(data):\n",
        "    data = sorted(data)\n",
        "    q1 = np.percentile(data, 25)\n",
        "    q3 = np.percentile(data, 75)\n",
        "    # print(q1, q3)\n",
        "    IQR = q3-q1\n",
        "    lwr_bound = q1-(1.5*IQR)\n",
        "    upr_bound = q3+(1.5*IQR)\n",
        "    # print(lwr_bound, upr_bound)\n",
        "    for i in data: \n",
        "        if (i<lwr_bound or i>upr_bound):\n",
        "            outliers.append(i)\n",
        "    return outliers# Driver code\n",
        "sample_outliers = detect_outliers_iqr(sample)\n",
        "print(\"Outliers from IQR method: \", sample_outliers)\n",
        "The above code outputs: Outliers from IQR method: [101]\n",
        "\n",
        "**5. Handling Outliers:**\n",
        "\n",
        "Till now we learned about detecting the outliers. The main question is how to deal with outliers?\n",
        "\n",
        "Below are some of the methods of treating the outliers\n",
        "\n",
        "1.Trimming/removing the outlier\n",
        "\n",
        "2.Quantile based flooring and capping\n",
        "\n",
        "3.Mean/Median imputation\n",
        "\n",
        "**5.1 Trimming/Remove the outliers:**\n",
        "\n",
        "In this technique, we remove the outliers from the dataset. Although it is not a good practice to follow.\n",
        "\n",
        "Python code to delete the outlier and copy the rest of the elements to another array.\n",
        "\n",
        "**Trimming**\n",
        "\n",
        "for i in sample_outliers:\n",
        "    a = np.delete(sample, np.where(sample==i))\n",
        "print(a)\n",
        "\n",
        "*print(len(sample), len(a))*\n",
        "\n",
        "The outlier ‘101’ is deleted and the rest of the data points are copied to another array ‘a’.\n",
        "\n",
        "**5.2 Quantile based flooring and capping:**\n",
        "\n",
        "In this technique, the outlier is capped at a certain value above the 90th percentile value or floored at a factor below the 10th percentile value.\n",
        "\n",
        "Python code:\n",
        "\n",
        "**Computing 10th, 90th percentiles and replacing the outliers**\n",
        "\n",
        "tenth_percentile = np.percentile(sample, 10)\n",
        "ninetieth_percentile = np.percentile(sample, 90)\n",
        "\n",
        "*print(tenth_percentile, ninetieth_percentile)*\n",
        "\n",
        "b = np.where(sample<tenth_percentile, tenth_percentile, sample)\n",
        "\n",
        "b = np.where(b>ninetieth_percentile, ninetieth_percentile, b)\n",
        "print(\"Sample:\", sample)\n",
        "\n",
        "print(\"New array:\",b)\n",
        "\n",
        "The above code outputs: New array: [15, 20.7, 18, 7.2, 13, 16, 11, 20.7, 7.2, 15, 10, 9]\n",
        "\n",
        "The data points that are lesser than the 10th percentile are replaced with the 10th percentile value and the data points that are greater than the 90th percentile are replaced with 90th percentile value.\n",
        "\n",
        "**5.3 Mean/Median imputation:**\n",
        "\n",
        "As the mean value is highly influenced by the outliers, it is advised to replace the outliers with the median value.\n",
        "\n",
        "Python Code:\n",
        "\n",
        "median = np.median(sample)# Replace with median\n",
        "for i in sample_outliers:\n",
        "    c = np.where(sample==i, 14, sample)\n",
        "print(\"Sample: \", sample)\n",
        "print(\"New array: \",c)\n",
        "\n",
        "*print(x.dtype)*\n",
        "\n",
        "**Visualizing the data after treating the outlier**\n",
        "\n",
        "plt.boxplot(c, vert=False)\n",
        "plt.title(\"Boxplot of the sample after treating the outliers\")\n",
        "plt.xlabel(\"Sample\")\n"
      ],
      "metadata": {
        "id": "wikG09BYOmIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature scaling Techniques:\n",
        "\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "In Data Processing, we try to change the data in such a way that the model can process it without any problems. And Feature Scaling is one such process in which we transform the data into a better version. Feature Scaling is done to normalize the features in the dataset into a finite range.\n",
        "\n",
        "I will be discussing why this is required and what are the common feature scaling techniques used.\n",
        "\n",
        "**Feature scaling techniques in python:**\n",
        "\n",
        "1.Absolute Maximum Scaling\n",
        "\n",
        "2.Min-Max Scaling\n",
        "\n",
        "3.Normalization\n",
        "\n",
        "4.Standardization\n",
        "\n",
        "5.Robust Scaling\n",
        "\n",
        "\n",
        "**Is Feature Scaling actually helpful?**\n",
        "\n",
        "**Why Feature Scaling?**\n",
        "\n",
        "Real Life Datasets have many features with a wide range of values like for example let’s consider the house price prediction dataset. It will have many features like no. of. bedrooms, square feet area of the house, etc.\n",
        "\n",
        "As you can guess, the no. of bedrooms will vary between 1 and 5, but the square feet area will range from 500-2000. This is a huge difference in the range of both features.\n",
        "\n",
        "Many machine learning algorithms that are using Euclidean distance as a metric to calculate the similarities will fail to give a reasonable recognition to the smaller feature, in this case, the number of bedrooms, which in the real case can turn out to be an actually important metric.\n",
        "\n",
        "Eg: Linear Regression, Logistic Regression, KNN\n",
        "\n",
        "There are several ways to do feature scaling. I will be discussing the top 5 of the most commonly used feature scaling techniques.\n",
        "\n",
        "1.Absolute Maximum Scaling\n",
        "\n",
        "2.Min-Max Scaling\n",
        "\n",
        "3.Normalization\n",
        "\n",
        "4.Standardization\n",
        "\n",
        "5.Robust Scaling\n",
        "\n",
        "\n",
        "**Absolute Maximum Scaling:**\n",
        "\n",
        "\n",
        "Find the absolute maximum value of the feature in the dataset\n",
        "\n",
        "Divide all the values in the column by that maximum value\n",
        "\n",
        "If we do this for all the numerical columns, then all their values will lie between -1 and 1. The main disadvantage is that the technique is sensitive to outliers. Like consider the feature *square feet*, if 99% of the houses have square feet area of less than 1000, and even if just 1 house has a square feet area of 20,000, then all those other house values will be scaled down to less than 0.05.\n",
        "\n",
        "I will be working with the sine and cosine functions throughout the article and show you how the scaling techniques affect their magnitude. sin() will be ranging between -1 and +1, and 50*cos() will be ranging between -50 and +50.\n",
        "\n",
        "\n",
        "This is how they actually look, you will not even be able to see that the red one is a sine graph, it basically looks like a straight squiggly line when compared to the big blue graph.\n",
        "\n",
        "y1_new = y1/max(y1)\n",
        "y2_new = y2/max(y2)\n",
        "Feature scaling techniques max scaling\n",
        "See from the graph that now both the datasets are ranging from -1 to +1 after the scaling.\n",
        "\n",
        "This might become significantly small with many data points below even 0.01 even if there is a single big outlier.\n",
        "\n",
        "**Min Max Scaling:**\n",
        "\n",
        "min-max you will subtract the minimum value in the dataset with all the values and then divide this by the range of the dataset(maximum-minimum). In this case, your dataset will lie between 0 and 1 in all cases whereas in the previous case, it was between -1 and +1. Again, this technique is also prone to outliers.\n",
        "\n",
        "y1_new = (y1-min(y1))/(max(y1)-min(y1))\n",
        "y2_new = (y2-min(y2))/(max(y2)-min(y2))\n",
        "plt.plot(x,y1_new,'red')\n",
        "plt.plot(x,y2_new,'blue')\n",
        "[<matplotlib.lines.Line2D at 0x7f6e1bf8fd30>]\n",
        "\n",
        "Feature scaling techniques min max scaled data\n",
        " \n",
        "\n",
        "**Normalization:**\n",
        "\n",
        "Instead of using the min() value in the previous case, in this case, we will be using the average() value.\n",
        "\n",
        "In scaling, you are changing the range of your data while in normalization you arere changing the shape of the distribution of your data.\n",
        "\n",
        "y1_new = (y1-np.mean(y1))/(max(y1)-min(y1))\n",
        "y2_new = (y2-np.mean(y2))/(max(y2)-min(y2))\n",
        "plt.plot(x,y1_new,'red')\n",
        "plt.plot(x,y2_new,'blue')\n",
        "[<matplotlib.lines.Line2D at 0x7f6e1bfb5518>]\n",
        "Feature scaling techniques Normalization\n",
        " \n",
        "\n",
        "**Standardization:**\n",
        "\n",
        "In standardization, we calculate the z-value for each of the data points and replaces those with these values.\n",
        "\n",
        "This will make sure that all the features are centred around the mean value with a standard deviation value of 1. This is the best to use if your feature is normally distributed like salary or age.\n",
        "\n",
        "y1_new = (y1-np.mean(y1))/np.std(y1)\n",
        "y2_new = (y2-np.mean(y2))/np.std(y2)\n",
        "plt.plot(x,y1_new,'red')\n",
        "plt.plot(x,y2_new,'blue')\n",
        "[<matplotlib.lines.Line2D at 0x7f6e25e66e10>]\n",
        "\n",
        " \n",
        "\n",
        "**Robust Scaling:**\n",
        "\n",
        "In this method, you need to subtract all the data points with the median value and then divide it by the Inter Quartile Range(IQR) value.\n",
        "\n",
        "Robust scaling\n",
        "IQR is the distance between the 25th percentile point and the 50th percentile point.\n",
        "\n",
        "This method centres the median value at zero and this method is robust to outliers.\n",
        "\n",
        "from scipy import stats \n",
        "IQR1 = stats.iqr(y1, interpolation = 'midpoint') \n",
        "y1_new = (y1-np.median(y1))/IQR1\n",
        "IQR2 = stats.iqr(y2, interpolation = 'midpoint') \n",
        "y2_new = (y2-np.median(y2))/IQR2\n",
        "plt.plot(x,y1_new,'red')\n",
        "plt.plot(x,y2_new,'blue')\n",
        "[<matplotlib.lines.Line2D at 0x7f6e25e19080>]\n",
        "Feature scaling techniques Robust scaling\n",
        " \n",
        "\n",
        "**Is Feature Scaling actually helpful?**\n",
        "\n",
        "Let’s look at an example of a College Admission dataset, in which your goal is to predict the chance of admission for each student based on the other features given.\n",
        "\n",
        "You can download the dataset from the link below.\n",
        "\n",
        "https://www.kaggle.com/mohansacharya/graduate-admissions\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"Admission_Predict.csv\")\n",
        "df.head()\n",
        "Admission Dataset\n",
        "The dataset has a wide variety of features with different ranges. The first column Serial No. is not important, so I am going to be deleting it. Then I am splitting the dataset into training and test dataset.\n",
        "\n",
        "df.drop(\"Serial No.\",axis=1,inplace=True)\n",
        "y = df['Chance of Admit ']\n",
        "df.drop(\"Chance of Admit \",axis=1,inplace=True)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(df,y,test_size=0.2)\n",
        "I am going to be building a linear regression model, first without normalization, and next with normalization, let’s check whether there is any improvement in the accuracy.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train,y_train)\n",
        "pred = lr.predict(x_test)\n",
        "from sklearn import metrics\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_test,pred))\n",
        "rmse\n",
        "0.06845052747026953\n",
        "See that without normalization the root mean squared error value comes out to be 0.0684, as most of the values in the `y` are less than 0.5.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(df)\n",
        "df = sc.transform(df)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(df,y,test_size=0.2)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train,y_train)\n",
        "pred = lr.predict(x_test)\n",
        "from sklearn import metrics\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_test,pred))\n",
        "rmse\n",
        "0.05674870151306346\n",
        "See that, we are able to get a significant reduction in the error when we used the standardization technique.\n",
        "\n"
      ],
      "metadata": {
        "id": "mMyolsx7yXuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Selection And Extraction Techniques :\n",
        "\n",
        "\n",
        "###**Feature Selection :**\n",
        "\n",
        "Feature selection involves selecting a subset of features from the original dataset that are most relevant to the prediction task. The goal of feature selection is to improve the performance of the machine learning model by reducing the dimensionality of the dataset, removing irrelevant or redundant features, and improving interpretability. Feature selection can be performed using various statistical and machine learning techniques, such as mutual information, correlation, or feature importance scores from a machine learning model.\n",
        "\n",
        "**There are several techniques used for feature selection:**\n",
        "\n",
        "**Filter methods:**\n",
        "\n",
        " Filter methods evaluate the relevance of features based on statistical measures, such as correlation or mutual information. They select features independently of the machine learning model and are often computationally efficient.\n",
        "\n",
        "**Wrapper methods:**\n",
        "\n",
        " Wrapper methods evaluate the performance of the machine learning model with different subsets of features. They are computationally expensive but can provide better results than filter methods.\n",
        "\n",
        "**Embedded methods:**\n",
        "\n",
        " Embedded methods perform feature selection as part of the machine learning model training process. These methods are often used with models that have built-in feature selection, such as Lasso or Ridge Regression.\n",
        "\n",
        "**Some common techniques used in feature selection include:**\n",
        "\n",
        "Correlation-based feature selection:\n",
        "\n",
        " Correlation-based feature selection measures the correlation between each feature and the target variable. Features with a high correlation are selected.\n",
        "\n",
        "Mutual information-based feature selection: \n",
        "\n",
        "Mutual information-based feature selection measures the dependence between each feature and the target variable. Features with a high mutual information score are selected.\n",
        "\n",
        "Recursive feature elimination: \n",
        "\n",
        "Recursive feature \n",
        "elimination is a wrapper method that recursively removes features and evaluates the performance of the machine learning model on the reduced feature set.\n",
        "\n",
        "Lasso regression: \n",
        "\n",
        "Lasso regression is an embedded method that performs feature selection by imposing a penalty on the absolute value of the coefficients. Features with small coefficients are set to zero and are removed from the model.\n",
        "\n",
        "###**Feature Extraction :**\n",
        "\n",
        "\n",
        "**What is Feature Extraction?**\n",
        "\n",
        "Feature extraction is a part of the dimensionality reduction process, in which, an initial set of the raw data is divided and reduced to more manageable groups. So when you want to process it will be easier. The most important characteristic of these large data sets is that they have a large number of variables. These variables require a lot of computing resources to process. So Feature extraction helps to get the best feature from those big data sets by selecting and combining variables into features, thus, effectively reducing the amount of data. These features are easy to process, but still able to describe the actual data set with accuracy and originality.\n",
        "\n",
        "\n",
        "**Why Feature Extraction is Useful?**\n",
        "\n",
        "The technique of extracting the features is useful when you have a large data set and need to reduce the number of resources without losing any important or relevant information. Feature extraction helps to reduce the amount of redundant data from the data set.\n",
        "\n",
        "In the end, the reduction of the data helps to build the model with less machine effort and also increases the speed of learning and generalization steps in the machine learning process.\n",
        "\n",
        "\n",
        "**There are several techniques used for feature extraction:**\n",
        "\n",
        "**Principal Component Analysis (PCA):**\n",
        "\n",
        " PCA is a linear transformation technique that identifies the directions in which the data varies the most and projects the data onto those directions. The new features are orthogonal and uncorrelated, and the first few principal components capture the most important information in the original dataset.\n",
        "\n",
        "**Linear Discriminant Analysis (LDA):**\n",
        "\n",
        " LDA is a supervised technique that maximizes the separation between the classes in the dataset. LDA identifies a set of features that maximize the ratio of between-class variance to within-class variance.\n",
        "\n",
        "**Non-negative matrix factorization (NMF):**\n",
        "\n",
        " NMF is a technique that factorizes a non-negative matrix into two non-negative matrices. NMF is often used for image and text data.\n",
        "\n",
        "**Autoencoder neural networks:**\n",
        "\n",
        " Autoencoder neural networks are neural networks that are trained to reconstruct the input data. The encoder part of the network learns a compressed representation of the input data, and the decoder part of the network reconstructs the original data.\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJ3yIwM2_WX9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mWGD8CFXm7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}